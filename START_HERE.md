# ğŸš€ START HERE

## Welcome to Chain-of-Thought Distillation Project

This is a **production-ready** project for training small language models to perform step-by-step reasoning using knowledge distillation from larger teacher models.

---

## âš¡ Quick Start (2 Commands)

### 1ï¸âƒ£ On Net Node (with internet) - ONE TIME
```bash
conda create -n cot_distill python=3.10 -y && conda activate cot_distill
conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia -y
pip install transformers peft datasets bitsandbytes accelerate tqdm sentencepiece protobuf deepspeed
cd /path/to/chain-of-thought-qwen
python scripts/download_models.py
```

### 2ï¸âƒ£ On GPU Node (offline) - RUN TRAINING
```bash
conda activate cot_distill
cd /path/to/chain-of-thought-qwen
./run_pipeline.sh
```

**Total Time: ~1.5 hours**

---

## ğŸ“š Documentation Guide

Choose your path:

### ğŸ¯ **I want to run it NOW**
â†’ Read: **ORDER_OF_EXECUTION.md**
- Step-by-step commands
- Copy-paste ready
- Verification steps

### ğŸ”§ **I need HPC cluster setup**
â†’ Read: **HPC_SETUP.md**
- Offline mode setup
- Multi-GPU configuration
- Shared filesystem guide

### âš¡ **I want a quick reference**
â†’ Read: **QUICK_REFERENCE.md**
- All commands in one page
- Monitoring tips
- Troubleshooting table

### ğŸ“– **I want complete documentation**
â†’ Read: **README.md**
- Full project details
- Hyperparameter guide
- Troubleshooting section

### ğŸ“ **I want to understand the project**
â†’ Read: **FINAL_SUMMARY.md**
- Project overview
- Technical specifications
- Expected results

---

## ğŸ“Š What You Get

- **Dataset**: 377 training + 95 test math questions
- **Models**: Qwen-14B (teacher) + Qwen-7B (student)
- **Method**: LoRA-based distillation
- **Output**: Trained model with 70-85% accuracy
- **Size**: LoRA adapters only ~50MB

---

## ğŸ¯ Project Structure

```
chain-of-thought-qwen/
â”œâ”€â”€ ğŸ“ configs/          # Multi-GPU training config
â”œâ”€â”€ ğŸ“ data/             # 377 train + 95 test questions
â”œâ”€â”€ ğŸ“ models/           # Teacher & student models (download first!)
â”œâ”€â”€ ğŸ“ scripts/          # All Python scripts
â”œâ”€â”€ ğŸ“ results/          # Training outputs
â”œâ”€â”€ ğŸš€ run_pipeline.sh   # Automated execution
â””â”€â”€ ğŸ“š Documentation files
```

---

## âœ… Prerequisites

- **Hardware**: 4x A100 GPUs (or 1x A100 40GB minimum)
- **Storage**: ~50GB free space
- **Network**: Offline GPU node + Online net node
- **Software**: Conda, CUDA 11.8+

---

## ğŸ”„ Execution Flow

```
Net Node (Internet):
  â””â”€> Download models (20-30 min)

GPU Node (Offline):
  â”œâ”€> Generate CoT (10-15 min, 1 GPU)
  â”œâ”€> Train Student (30-45 min, 4 GPUs)
  â””â”€> Evaluate (2-3 min, 1 GPU)
```

---

## ğŸ“ Key Files

| File | Purpose |
|------|---------|
| **START_HERE.md** | ğŸ‘ˆ You are here |
| **ORDER_OF_EXECUTION.md** | Exact execution order |
| **QUICK_REFERENCE.md** | All commands, one page |
| **HPC_SETUP.md** | Cluster-specific setup |
| **FINAL_SUMMARY.md** | Complete overview |
| **run_pipeline.sh** | Automated script |

---

## ğŸ¬ What Happens When You Run

### Stage 1: Generate CoT Dataset
- Teacher model reads 377 questions
- Generates step-by-step reasoning
- Saves to `data/cot_dataset.jsonl`

### Stage 2: Train Student Model
- Student model learns from CoT examples
- Uses LoRA (efficient fine-tuning)
- Saves weights to `results/student_cot_lora/`

### Stage 3: Evaluate
- Tests on 95 held-out questions
- Calculates accuracy
- Saves predictions to `results/predictions.jsonl`

---

## ğŸ› ï¸ Troubleshooting

| Problem | Solution |
|---------|----------|
| No internet on GPU node | Download models on net node first |
| CUDA out of memory | Reduce `--batch_size` to 4 or 2 |
| Module not found | Activate conda environment |
| Models not found | Run `download_models.py` on net node |

---

## ğŸ“ Need Help?

1. **Quick issue?** â†’ Check **QUICK_REFERENCE.md** troubleshooting table
2. **Setup problem?** â†’ Read **HPC_SETUP.md** section 3
3. **Execution error?** â†’ Follow **ORDER_OF_EXECUTION.md** step-by-step
4. **Understanding?** â†’ Read **FINAL_SUMMARY.md**

---

## ğŸ¯ Next Steps

1. **Read**: ORDER_OF_EXECUTION.md
2. **Setup**: Download models on net node
3. **Run**: Execute pipeline on GPU node
4. **Verify**: Check results in `results/predictions.jsonl`

---

## ğŸ“ˆ Expected Results

After successful execution:
- âœ… Trained LoRA model (~50MB)
- âœ… 95 predictions with accuracy score
- âœ… 70-85% accuracy on math problems
- âœ… Student model can do step-by-step reasoning

---

## ğŸš€ Ready to Start?

**â†’ Go to ORDER_OF_EXECUTION.md and follow the steps!**

---

## ğŸ“ Quick Command Summary

```bash
# On net node (once)
python scripts/download_models.py

# On GPU node (every time)
./run_pipeline.sh

# That's it!
```

---

**Good luck! ğŸ‰**
